import logging
import os
import platform
import re
import socket
from datetime import timedelta
from time import sleep
from typing import List
from urllib.parse import urlparse, urlunparse
from urllib.request import Request, urlopen, HTTPError, URLError

import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from requests.utils import quote as _quote

from speasy import __version__
from speasy.core.cache import CacheCall

log = logging.getLogger(__name__)

USER_AGENT = f'Speasy/{__version__} {platform.uname()} (SciQLop project)'

DEFAULT_TIMEOUT = 60  # seconds

DEFAULT_DELAY = 5  # seconds

DEFAULT_RETRY_COUNT = 5

STATUS_FORCE_LIST = [500, 502, 504]

RETRY_AFTER_LIST = [429, 503]  # Note: Specific treatment for 429 & 503 error codes (see below)

_HREF_REGEX = re.compile(' href="([A-Za-z0-9.-_]+)">')


def ensure_url_scheme(url: str) -> str:
    """Adds file:// to url for local files

    Parameters
    ----------
    url : str
        url with or without scheme
    Returns
    -------
    str
        url with 'file:' scheme added when none was provided else input url
    """
    parsed = urlparse(url)
    if parsed.scheme == '':
        return urlunparse(parsed._replace(scheme='file'))
    return url


class TimeoutHTTPAdapter(HTTPAdapter):
    def __init__(self, *args, timeout=DEFAULT_TIMEOUT, **kwargs):
        self.timeout = timeout
        super().__init__(*args, **kwargs)

    def send(self, request, **kwargs):
        kwargs.pop('timeout', None)
        return super().send(request, timeout=self.timeout, **kwargs)


def quote(*args, **kwargs):
    return _quote(*args, **kwargs)


def apply_delay(headers: dict = None):
    delay = DEFAULT_DELAY
    try:
        if headers and ('Retry-After' in headers):
            delay = float(headers['Retry-After'])
    except ValueError:
        pass
    log.debug(f"Will sleep for {delay} seconds")
    sleep(delay)


def get(url, headers: dict = None, params: dict = None, timeout: int = DEFAULT_TIMEOUT, head_only: bool = False):
    headers = {} if headers is None else headers
    headers['User-Agent'] = USER_AGENT
    # cf. https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/
    retry_strategy = Retry(
        total=DEFAULT_RETRY_COUNT,
        backoff_factor=1,
        status_forcelist=STATUS_FORCE_LIST,
        allowed_methods=["HEAD", "GET"]
    )
    adapter = TimeoutHTTPAdapter(max_retries=retry_strategy, timeout=timeout)
    http = requests.Session()
    http.mount("https://", adapter)
    http.mount("http://", adapter)
    while True:
        if head_only:
            resp = http.head(url, headers=headers, params=params)
        else:
            resp = http.get(url, headers=headers, params=params)
        if resp.status_code in RETRY_AFTER_LIST:  # Honor "Retry-After"
            log.debug(f"Got {resp.status_code} response")
            apply_delay(resp.headers)
        else:
            break
    return resp


def urlopen_with_retry(url, timeout: int = DEFAULT_TIMEOUT, headers: dict = None):
    headers = {} if headers is None else headers
    headers['User-Agent'] = USER_AGENT
    req = Request(ensure_url_scheme(url), headers=headers)
    retrycount = 0
    while True:
        try:
            resp = urlopen(req, timeout=timeout)
            return resp
        except HTTPError as e:
            if isinstance(e.reason, socket.timeout):
                log.debug("Timeout exception during urlopen request")
            elif e.code in STATUS_FORCE_LIST:
                log.debug(f"HTTP Error Got {e.code} response")
            elif e.code in RETRY_AFTER_LIST:  # Honor "Retry-After"
                log.debug(f"Got {e.code} response")
                apply_delay(resp.headers)
                return urlopen_with_retry(url, timeout=timeout, headers=headers)
            else:
                raise e
            retrycount += 1
            if retrycount > DEFAULT_RETRY_COUNT:
                raise e
            apply_delay()
        except URLError as e:
            log.debug(f"Got {e.reason} error")
            retrycount += 1
            if retrycount > DEFAULT_RETRY_COUNT:
                raise e
            apply_delay()


def _list_local_files(path: str) -> List[str]:
    return os.listdir(path)


@CacheCall(cache_retention=timedelta(hours=12), is_pure=True)
def _list_remote_files(url: str) -> List[str]:
    response = get(url)
    if response.ok:
        return _HREF_REGEX.findall(response.text)
    return []


def list_files(url: str, file_regex: re.Pattern) -> List[str]:
    """If url scheme is WEB then Lists matching downloadable files inside an html page generated by Apache mod_dir or equivalent, else if url scheme is local files it will list matching files in given path.

    Parameters
    ----------
    url : str
        WEB url or local path to scan
    file_regex : re.Pattern
        regex used to filter files

    Returns
    -------
    List[str]
        matching remote or local files
    """
    split_url = urlparse(url)
    if split_url.scheme in ('', 'file'):
        files = _list_local_files(split_url.path)
    else:
        files = _list_remote_files(url)
    return list(filter(file_regex.match, files))
